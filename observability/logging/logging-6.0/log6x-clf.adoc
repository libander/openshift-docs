:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="log6x-CLF_{context}"]
= Understanding the ClusterLogForwarder Custom Resource
:context: logging-6x

toc::[]

The `ClusterLogForwarder` (CLF) allows users to configure forwarding of logs to various destinations. It provides a flexible way to select log messages from different sources, send them through a pipeline that can transform or filter them, and forward them to one or more outputs.

.Key Functions of the ClusterLogForwarder
* Selects log messages using inputs
* Forwards logs to external destinations using outputs
* Filters, transforms, and drops log messages using filters
* Defines log forwarding pipelines connecting inputs, filters and outputs
* Provides global configuration options

== Structuring the ClusterLogForwarder

The CLF has a `spec` section that contains the following key components:

Inputs:: Select log messages to be forwarded. Built-in input types `application`, `infrastructure` and `audit` forward logs from different parts of the cluster. You can also define custom inputs.

Outputs:: Define destinations to forward logs to. Each output has a unique name and type-specific configuration.

Pipelines:: Define the path logs take from inputs, through filters, to outputs. Pipelines have a unique name and consist of a list of input, output and filter names.

Filters:: Transform or drop log messages in the pipeline. Users can define filters that match certain log fields and drop or modify the messages. Filters are applied in the order specified in the pipeline.

== Configuring Inputs

Inputs are configured in an array under `spec.inputs`. There are three built-in input types:

application:: Selects logs from all application containers, except those in infrastructure namespaces like `openshift-*` and `kube-*`.

infrastructure:: Selects logs from infrastructure components running in `openshift-*` and `kube-*` namespaces and node logs.

audit:: Selects logs from the OpenShift API server audit logs, Kubernetes API server audit logs and node audit logs from auditd.

Users can define custom inputs of type `application` that select logs from specific namespaces or using pod labels.

The `inputDefaults` field sets global options for inputs like log-level filtering.

== Configuring Outputs

Outputs are configured in an array under `spec.outputs`. Each output must have a unique name and a type. Supported types include:

elasticsearch:: Forwards logs to an external Elasticsearch instance.
fluentdForward:: Forwards logs to an external Fluentd instance.
syslog:: Forwards logs to an external syslog server.
kafka:: Forwards logs to a Kafka broker.
loki:: Forwards logs to a Loki logging backend.
cloudwatch:: Forwards logs to AWS Cloudwatch.

Each output type has its own configuration fields. Common ones include:

url:: URL of the output destination.
secret:: Name of an OpenShift secret containing authentication credentials for the output.
tls:: TLS client configuration for the output.

The `outputDefaults` field sets global options for outputs like buffer parameters.

== Configuring Pipelines

Pipelines are configured in an array under `spec.pipelines`. Each pipeline must have a unique name and consists of:

inputRefs:: Names of inputs whose logs should be forwarded to this pipeline.
outputRefs:: Names of outputs to send logs to.
filterRefs:: (optional) Names of filters to apply.

The order of filterRefs matters, as they are applied sequentially. Earlier filters can drop messages that will not be processed by later filters.

== Configuring Filters

Filters are configured in an array under `spec.filters`. They can match incoming log messages based on the value of structured fields and modify or drop them.

Users can configure the following types of filters:

labelFilter:: Adds labels to logs matching the filter. Useful for labeling certain types of logs.
anonymizeFilter:: Masks user-sensitive fields using hash functions. Useful for GDPR compliance.
nullFilter:: Drops logs matching the filter.

Each filter type has a `match` configuration for matching fields and filter-specific parameters.

== Enabling Output Security

Outputs can be configured to use TLS/SSL by specifying a `secret` containing the TLS certificate and key. The secret can be created using:

[source,terminal]
----
$ oc create secret tls my-es-secret --cert=my-es.crt --key=my-es.key
----

Then reference it in the output configuration:

[source,yaml]
----
spec:
  outputs:
    - name: my-es
      type: elasticsearch
      url: https://elasticsearch.example.com
      secret:
        name: my-es-secret
----

For authentication, outputs can specify a `secret` containing username/password or token credentials:

[source,terminal]
----
$ oc create secret generic my-es-secret
  --from-literal=username=my-username 
  --from-literal=password=my-password
----

Then reference it in the output configuration:

[source,yaml]
----
spec:
  outputs:
    - name: my-es
      type: elasticsearch
      url: https://elasticsearch.example.com
      secret:
        name: my-es-secret
----

Additional output-specific authentication methods are also supported, like the shared_key for Fluentd.

== Managing the Operator

The `ClusterLogForwarder` resource has a `managementState` field that controls whether the operator actively manages its resources or leaves them unmanaged:

managed:: (default) The operator will drive the logging resources to match the desired state in the CLF spec.

unmanaged:: The operator will not take any action related to the logging components.

This allows administrators to temporarily pause log forwarding by setting `managementState` to `unmanaged`.

== Configuring Global Options

The following fields set global configuration options for the CLF:

inputDefaults:: Options like log-level filtering and parsing that apply to all inputs.
outputDefaults:: Options like buffer and retry parameters that apply to all outputs.
collectCollector.enable:: Enables collection of logs from the log collectors themselves.
